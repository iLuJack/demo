# app.py

import streamlit as st
import os
import sys

# Adjust import paths for Streamlit Cloud
from document_loader import process_documents
from embedding_store import create_vector_store, load_vector_store
from cloud_rag_chatbot import setup_rag_system, ask_question

from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šé‡
load_dotenv()

# è¨­ç½®é é¢é…ç½®å’Œæ¨™é¡Œ
st.set_page_config(
    page_title="å°ç£æ³•å¾‹åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="wide"
)

st.title("å°ç£æ³•å¾‹åŠ©æ‰‹ ğŸ‡¹ğŸ‡¼âš–ï¸")
st.markdown("*åŸºæ–¼ RAG æŠ€è¡“çš„å°ç£æ³•å¾‹å•ç­”ç³»çµ±*")

# å´é‚Šæ¬„é…ç½®
with st.sidebar:
    st.header("ç³»çµ±è¨­ç½®")
    
    # é‡å»ºå‘é‡å­˜å„²é¸é …
    rebuild_db = st.checkbox("é‡å»ºå‘é‡æ•¸æ“šåº«", value=False, 
                           help="é¸ä¸­æ­¤é …å°‡é‡æ–°è™•ç†æ–‡æª”ä¸¦å‰µå»ºæ–°çš„å‘é‡å­˜å„²")
    
    # é¡¯ç¤ºä¾†æºæ–‡æª”é¸é …
    show_sources = st.checkbox("é¡¯ç¤ºä¾†æºæ–‡æª”", value=True,
                             help="é¡¯ç¤ºå›ç­”çš„ä¾†æºæ–‡æª”")
    
    # æ¨¡å‹é¸æ“‡
    model_option = st.selectbox(
        "é¸æ“‡ LLM æ¨¡å‹",
        [
            "google/flan-t5-large",  # å¤šèªè¨€æ¨¡å‹
            "bigscience/bloom-1b7",  # æ”¯æŒä¸­æ–‡
            "BAAI/bge-large-zh-v1.5",  # ä¸­æ–‡æ¨¡å‹
            "fnlp/bart-base-chinese"  # ä¸­æ–‡æ¨¡å‹
        ],
        index=0
    )
    
    # æº«åº¦è¨­ç½®
    temperature = st.slider("æº«åº¦", min_value=0.0, max_value=1.0, value=0.2, step=0.1,
                          help="è¼ƒä½çš„å€¼ä½¿å›ç­”æ›´ç¢ºå®šï¼Œè¼ƒé«˜çš„å€¼ä½¿å›ç­”æ›´å¤šæ¨£åŒ–")
    
    # æª¢ç´¢æ–‡æª”æ•¸é‡
    k_docs = st.slider("æª¢ç´¢æ–‡æª”æ•¸é‡", min_value=1, max_value=5, value=2, step=1,
                      help="å¾å‘é‡å­˜å„²ä¸­æª¢ç´¢çš„æ–‡æª”æ•¸é‡")
    
    # æ·»åŠ é‡ç½®æŒ‰éˆ•
    if st.button("é‡ç½®èŠå¤©"):
        st.session_state.messages = []
        st.session_state.sources = []
        st.experimental_rerun()
    
    st.markdown("---")
    st.markdown("### é—œæ–¼")
    st.markdown("æ­¤æ‡‰ç”¨ä½¿ç”¨ HuggingFace APIã€LangChain å’Œ Streamlit æ§‹å»º")
    st.markdown("Â© 2023 å°ç£æ³•å¾‹åŠ©æ‰‹åœ˜éšŠ")

# æª¢æŸ¥ API å¯†é‘°
if not os.getenv("HUGGINGFACE_API_TOKEN"):
    st.error("è«‹è¨­ç½® HUGGINGFACE_API_TOKEN ç’°å¢ƒè®Šé‡æˆ–åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ ")
    st.stop()

# åˆå§‹åŒ–èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state.messages = []

if "sources" not in st.session_state:
    st.session_state.sources = []

# åˆå§‹åŒ– RAG ç³»çµ±
@st.cache_resource(show_spinner="æ­£åœ¨åŠ è¼‰ RAG ç³»çµ±...")
def load_rag(rebuild=False, model_name="google/flan-t5-large", temp=0.2, k=2):
    with st.spinner("æ­£åœ¨è¨­ç½® RAG ç³»çµ±ï¼Œé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜..."):
        return setup_rag_system(
            rebuild_vector_store=rebuild,
            model_name=model_name,
            temperature=temp,
            k=k
        )

# åŠ è¼‰ RAG ç³»çµ±
rag_chain = load_rag(
    rebuild=rebuild_db, 
    model_name=model_option, 
    temp=temperature, 
    k=k_docs
)

# é¡¯ç¤ºèŠå¤©æ­·å²
chat_container = st.container()
with chat_container:
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # é¡¯ç¤ºä¾†æºï¼ˆå¦‚æœæœ‰ä¸”ç”¨æˆ¶é¸æ“‡é¡¯ç¤ºï¼‰
            if show_sources and message["role"] == "assistant" and i < len(st.session_state.sources):
                sources = st.session_state.sources[i]
                if sources:
                    with st.expander("æŸ¥çœ‹ä¾†æºæ–‡æª”"):
                        for j, source in enumerate(sources):
                            st.markdown(f"**ä¾†æº {j+1}**: {source['source']}")
                            st.markdown(f"```\n{source['content'][:300]}...\n```")

# ç²å–ç”¨æˆ¶è¼¸å…¥
user_input = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„æ³•å¾‹å•é¡Œ...")

# è™•ç†ç”¨æˆ¶è¼¸å…¥
if user_input:
    # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯åˆ°èŠå¤©æ­·å²
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # é¡¯ç¤ºåŠ©æ‰‹æ¶ˆæ¯
    with st.chat_message("assistant"):
        # å‰µå»ºä¸€å€‹ç©ºçš„ä½”ä½ç¬¦
        message_placeholder = st.empty()
        
        # é¡¯ç¤ºæ€è€ƒä¸­çš„æŒ‡ç¤º
        thinking_msg = st.empty()
        thinking_msg.markdown("*æ€è€ƒä¸­...*")
        
        # ç²å–å›ç­”
        try:
            response = rag_chain.invoke({"query": user_input})
            answer = response["result"]
            source_docs = response["source_documents"]
            
            # é¡¯ç¤ºå›ç­”
            message_placeholder.markdown(answer)
            thinking_msg.empty()
            
            # è™•ç†ä¾†æºæ–‡æª”
            if source_docs and show_sources:
                sources_data = []
                with st.expander("æŸ¥çœ‹ä¾†æºæ–‡æª”"):
                    for i, doc in enumerate(source_docs):
                        st.markdown(f"**ä¾†æº {i+1}**: {doc.metadata['source']}")
                        st.markdown(f"```\n{doc.page_content[:300]}...\n```")
                        sources_data.append({
                            "source": doc.metadata['source'],
                            "content": doc.page_content
                        })
                
                # ä¿å­˜ä¾†æºä»¥ä¾¿æ­·å²é¡¯ç¤º
                st.session_state.sources.append(sources_data)
            else:
                st.session_state.sources.append([])
            
            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°èŠå¤©æ­·å²
            st.session_state.messages.append({"role": "assistant", "content": answer})
            
        except Exception as e:
            error_msg = f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            message_placeholder.error(error_msg)
            thinking_msg.empty()
            
            # æ·»åŠ éŒ¯èª¤æ¶ˆæ¯åˆ°èŠå¤©æ­·å²
            st.session_state.messages.append({"role": "assistant", "content": error_msg})
            st.session_state.sources.append([])